{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "645de3d9-6c46-487d-8758-34140725cd24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SalesLT_ProductCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefaf92a-8dbc-4e6d-9326-4fad316cb7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable auto merge\n",
    "\n",
    "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d79c5e-d528-4b82-ad05-262d3be3ec72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Utils/Utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3e5aad-2832-416b-94b4-e117efceeef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType, StringType, TimestampType, StructType, StructField\n",
    ")\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.functions import count, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0b21bd-eb5e-4b53-99f0-18b4d494c1c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading Table\n",
    "\n",
    "\n",
    "df = spark.table(\"adlslmcompany_bronze.managed_bronze.saleslt_productcategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b49770-4965-445f-afed-88d63c3b0f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Displaying table\n",
    "\n",
    "df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d31219-7c29-44e6-9c5a-553a96d79e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking for duplicated values\n",
    "\n",
    "checkduplicates(df, \"ProductCategoryID\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa66ae9-84bd-4c7f-a2a9-80b96478c651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def silver_clean_productcategory(df): \n",
    "\n",
    "    # Adds transformation Date column\n",
    "    df = df.withColumn(\"silves_transformed_timestamp\", F.current_timestamp())\n",
    "\n",
    "\n",
    "        #Cast to ensure datatype\n",
    "    df = df.select(\n",
    "         F.col('ProductCategoryID').cast(IntegerType()).alias('ProductCategoryID'),\n",
    "         F.col('ParentProductCategoryID').cast(IntegerType()).alias('ParentProductCategoryID'),\n",
    "         F.col('Name').cast(StringType()).alias('Name'),\n",
    "         F.col('rowguid').cast(StringType()).alias('rowguid'),\n",
    "         F.col('ModifiedDate').cast(TimestampType()).alias('ModifiedDate'),\n",
    "         F.col('bronze_ingestion_timestamp').cast(TimestampType()).alias('bronze_ingestion_timestamp'),\n",
    "         F.col('silves_transformed_timestamp').cast(TimestampType()).alias('silves_transformed_timestamp'),\n",
    "                 )\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb539dc-5b64-4ea3-be33-bb5fc65ca623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Defining expected schema\n",
    "expected_schema = StructType([\n",
    "    StructField(\"ProductCategoryID\", IntegerType(), False), \n",
    "    StructField(\"ParentProductCategoryID\", IntegerType(), False),\n",
    "    StructField(\"Name\", StringType(), False),\n",
    "    StructField(\"rowguid\", StringType(), False),\n",
    "    StructField(\"ModifiedDate\", TimestampType(), False),\n",
    "    StructField(\"bronze_ingestion_timestamp\", TimestampType(), False),\n",
    "    StructField(\"silves_transformed_timestamp\", TimestampType(), False)      \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48551fc-84e2-43b0-b544-b736d0ec85f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transforming\n",
    "\n",
    "silver_df = silver_clean_productcategory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc0574b-c0ee-4791-a79b-0ef39120b68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Comparing lenghts\n",
    "\n",
    "compare_lengths(df, silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3a337b-d173-4632-9cd4-17f9f6057019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the schema \n",
    "_validate_schema(silver_df, expected_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f9389a-333c-4110-9843-6c8dc8596c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANT: Please note that this is a simulated project; the upsert operation will be executed within this notebook. In a production environment, a dedicated notebook containing only the function and validations would be developed. All function notebooks would be orchestrated by Azure Data Factory (ADF) pipelines or Azure Databricks (ADB) workflows. The method of upsert may vary based on the utilization of auto loader, streaming, or Change Data Feed (CDF).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abea014-809a-4cb4-a609-ec984741af04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading into the Silver Layer  \n",
    "\n",
    "target_table= \"saleslt_productcategory\"   \n",
    "\n",
    "schema = \"managed_silver\"\n",
    "\n",
    "catalog = \"adlslmcompany_silver\"\n",
    "\n",
    "primary_keys = [\"ProductCategoryID\"]\n",
    "\n",
    "\n",
    "_upsert_silver_table(silver_df, target_table, primary_keys, schema, catalog )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Saleslt_ProductCategory",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
