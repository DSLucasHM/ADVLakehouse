{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3982af1-3aa4-455b-a377-1a0fd980bd1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Notebook used to storage multi purpose use functions on the silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2400a46e-fa6b-403c-82c0-5c16bdfdfbe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkduplicates(df, idcol):\n",
    "    duplicated = df.groupBy(col(idcol)).agg(count(col(idcol)).alias(\"count\")).filter(col(\"count\") > 1)\n",
    "    if duplicated.count() > 0:\n",
    "        return duplicated.display()\n",
    "    else:\n",
    "         return print(\"No duplicated values found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ad25ce-f1cd-4c2d-b1e4-bfae84b25824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def deduplicate(df, idcol, timecol=None):\n",
    "    if timecol:\n",
    "        window_spec = Window.partitionBy(idcol).orderBy(col(timecol).desc())\n",
    "        df = df.withColumn('row_num', row_number().over(window_spec))\n",
    "        df = df.filter(col('row_num') == 1).drop('row_num')\n",
    "    else:\n",
    "        df = df.dropDuplicates([idcol])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e80d70-cbc3-446c-9e57-b62f6ef9163e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def graphbycolumnd (df,col): \n",
    "    df= df.groupBy(col).agg(count(col).alias(\"count\")).orderBy(desc(\"count\"))\n",
    "\n",
    "    return df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85116fb0-56e1-4a70-8989-45286f35d04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def iqr_outlier(dataframe, column, fac=1.5):\n",
    "\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    quantiles = dataframe.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1\n",
    "\n",
    "     # Define the upper and lower bounds for outliers\n",
    "    lower_bound = q1 - fac * iqr\n",
    "    upper_bound = q3 + fac * iqr\n",
    "\n",
    "    # Filter outliers and update the DataFrame\n",
    "    outliers_df = dataframe.filter((col(column) < lower_bound) | (col(column) > upper_bound) | (col(column) < 1))\n",
    "    \n",
    "    if not outliers_df.rdd.isEmpty():\n",
    "        print(\"Outlier found\")\n",
    "        outliers_df.display()\n",
    "        cleaned_df = dataframe.filter((col(column) >= lower_bound) & (col(column) <= upper_bound) & (col(column) >= 1))\n",
    "        return cleaned_df, outliers_df\n",
    "    else:\n",
    "        print(\"No outlier\")\n",
    "        return dataframe, outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93a3cf5-1399-4527-8ce2-02dddb7058ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _validate_schema(df, expected_schema) -> bool:\n",
    "\n",
    "    actual_schema = df.schema\n",
    "\n",
    "    # Veryfing the number of columns\n",
    "    if len(expected_schema.fields) != len(actual_schema.fields):\n",
    "        return False\n",
    "\n",
    "    # Verfying the data types and names\n",
    "    for i, field in enumerate(actual_schema.fields):\n",
    "        expected_field = expected_schema.fields[i]\n",
    "        if field.name != expected_field.name or not isinstance(field.dataType, type(expected_field.dataType)):\n",
    "            print(f\"Error: {field.name}\")\n",
    "            print(f\"Expected: {expected_field}, Find: {field}\")\n",
    "            return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4fff7d-8bf9-405c-b9b1-411acc4235d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "\n",
    "def upsert_table(transformed_df: DataFrame, target_table: str, primary_keys: list, schema: str, catalog: str, not_matched_by_source_action: str = None, not_matched_by_source_condition: str = None):\n",
    "    \n",
    "    # Construct the full table name with catalog and schema\n",
    "    full_table_name = f\"{catalog}.{schema}.{target_table}\"\n",
    "    \n",
    "    # Verifying landing table existence\n",
    "    if not spark.catalog.tableExists(full_table_name):\n",
    "        # If not, create it using primary key as columns \n",
    "        transformed_df.write.format(\"delta\").saveAsTable(full_table_name)\n",
    "        print(f\"Table {full_table_name} created.\")\n",
    "        return\n",
    "    \n",
    "    # Creating merge condition\n",
    "    merge_condition = \" AND \".join([f\"s.{key} = t.{key}\" for key in primary_keys])\n",
    "\n",
    "    # Load existing delta table as a DataFrame\n",
    "    delta_table = DeltaTable.forName(spark, full_table_name)\n",
    "\n",
    "    # Initiate merge operation\n",
    "    merge_builder = delta_table.alias(\"t\").merge(\n",
    "        transformed_df.alias(\"s\"),\n",
    "        merge_condition\n",
    "    )\n",
    "\n",
    "    # When matched clause\n",
    "    merge_builder = merge_builder.whenMatchedUpdateAll()\n",
    "\n",
    "    # When not matched clause\n",
    "    merge_builder = merge_builder.whenNotMatchedInsertAll()\n",
    "\n",
    "    # If needed, use the not_matched_by_source_action as DELETE\n",
    "    if not_matched_by_source_action and not_matched_by_source_action.upper() == \"DELETE\":\n",
    "        unmatched_rows = delta_table.toDF().alias(\"t\").join(\n",
    "            transformed_df.alias(\"s\"),\n",
    "            on=[F.col(f\"t.{key}\") == F.col(f\"s.{key}\") for key in primary_keys],\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        # Apply the condition if needed\n",
    "        if not_matched_by_source_condition:\n",
    "            unmatched_rows = unmatched_rows.filter(not_matched_by_source_condition)\n",
    "\n",
    "        delta_table.alias(\"t\").merge(\n",
    "            unmatched_rows.alias(\"s\"),\n",
    "            merge_condition\n",
    "        ).whenMatchedDelete().execute()\n",
    "\n",
    "    # Execute merge\n",
    "    merge_builder.execute()\n",
    "    \n",
    "    print(\"Upsert executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08410921-aff2-46b8-9d1f-bf4f7a1c8639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def compare_lengths(df1, df2):\n",
    "\n",
    "    len_df1 = df1.count()\n",
    "    len_df2 = df2.count()\n",
    "    \n",
    "    if len_df1 == len_df2:\n",
    "        return f\"Both DataFrames have the same number of rows: {len_df1}\"\n",
    "    else:\n",
    "        return f\"The DataFrames have different numbers of rows: {len_df1} and {len_df2}\"\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
